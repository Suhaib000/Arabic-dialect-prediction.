{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Arabic dialect Prediction","metadata":{}},{"cell_type":"markdown","source":"This notebook aims to build a model that predicts the dialect given the text.First by attempting some classical ML models .Then  moving to deep learning approach through finetuning an [Multi-dialect-Arabic-BERT](https://github.com/mawdoo3/Multi-dialect-Arabic-BERT)  trained on 10M arabic tweets.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents:\n* [Classic ML Approach](#1)\n    * [Preprocessing](#1.1)\n    * [Evaluate best classifier's performance on other datasets](#1.2)\n    * [Summary of classic ML results](#1.3)\n* [Deep Learning Approach](#2)\n * [Multi-dialect-Arabic-BERT](#2.1)\n     * [Preprocessing](#2.1.1)\n     * [Create data loaders for test and validation sets](#2.1.2)\n     * [Define model initialization class and functions](#2.1.3)\n     * [Define model train and evaluate functions](#2.1.4)\n     * [Initialize and train model](#2.1.5)\n     * [Save model](#2.1.6)\n     * [Define prediction and test set evaluation functions](#2.1.7)\n     * [Predict and evaluate validation subset](#2.1.8)\n     * [Predict and evaluate test subset](#2.1.9)\n     * [Summary of performance on test datasets](#2.1.10)\n\n* [Summary](#3)\n","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\n\n%matplotlib inline","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-14T16:00:18.034993Z","iopub.execute_input":"2022-03-14T16:00:18.035335Z","iopub.status.idle":"2022-03-14T16:00:18.042068Z","shell.execute_reply.started":"2022-03-14T16:00:18.035299Z","shell.execute_reply":"2022-03-14T16:00:18.040984Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Load dataset ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nNEW_DF = pd.read_csv('../input/dataset/Data.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:19.105787Z","iopub.execute_input":"2022-03-14T16:00:19.106105Z","iopub.status.idle":"2022-03-14T16:00:20.781955Z","shell.execute_reply.started":"2022-03-14T16:00:19.106075Z","shell.execute_reply":"2022-03-14T16:00:20.781087Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"> <a id=\"1\"></a>\n# Classical ML approach\n","metadata":{}},{"cell_type":"markdown","source":"**Exploratory data analysis(EDA)**","metadata":{}},{"cell_type":"code","source":"NEW_DF.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:23.299219Z","iopub.execute_input":"2022-03-14T16:00:23.299550Z","iopub.status.idle":"2022-03-14T16:00:23.308159Z","shell.execute_reply.started":"2022-03-14T16:00:23.299517Z","shell.execute_reply":"2022-03-14T16:00:23.307367Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"NEW_DF.dialect.unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:24.197435Z","iopub.execute_input":"2022-03-14T16:00:24.197812Z","iopub.status.idle":"2022-03-14T16:00:24.238466Z","shell.execute_reply.started":"2022-03-14T16:00:24.197756Z","shell.execute_reply":"2022-03-14T16:00:24.237597Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dialect_count = NEW_DF.groupby('dialect', as_index=False).count()\ndialect_count.sort_values(['text'],ascending=False,)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:24.649743Z","iopub.execute_input":"2022-03-14T16:00:24.650220Z","iopub.status.idle":"2022-03-14T16:00:24.836284Z","shell.execute_reply.started":"2022-03-14T16:00:24.650173Z","shell.execute_reply":"2022-03-14T16:00:24.835353Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nFig1 =px.bar(x=dialect_count.dialect ,y=dialect_count.text, template='plotly_dark')\n\nFig1.update_traces(  texttemplate=\"%{y}\",textposition='outside')\nFig1.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:30.066915Z","iopub.execute_input":"2022-03-14T16:00:30.067233Z","iopub.status.idle":"2022-03-14T16:00:33.946797Z","shell.execute_reply.started":"2022-03-14T16:00:30.067203Z","shell.execute_reply":"2022-03-14T16:00:33.946006Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"* our data is Imbalanced and that can cause a lot of frustration.\n* we can try Resampling our Dataset","metadata":{}},{"cell_type":"code","source":"#using over-sampling method that can add more copies to the minority class.\n\nunder_balanced_df = pd.DataFrame()\nunder_balanceed_label =['OM','SY','DZ','IQ','SD','MA','YE','TN']\n\nfor lebel in under_balanceed_label:\n    under_balanced_df=under_balanced_df.append(NEW_DF[NEW_DF.dialect==lebel].sample(25000, replace=True)).reset_index(drop=True)\nunder_balanced_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:33.948691Z","iopub.execute_input":"2022-03-14T16:00:33.949231Z","iopub.status.idle":"2022-03-14T16:00:34.494416Z","shell.execute_reply.started":"2022-03-14T16:00:33.949191Z","shell.execute_reply":"2022-03-14T16:00:34.493578Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#using under-sampling method that can delete instances from the over-represented class.\nover_balanceed_label=['LY', 'QA', 'PL', 'JO','SA','EG','LB','KW','AE','BH']\nover_balanced_df = pd.DataFrame()\n\nfor lebel in over_balanceed_label:\n    over_balanced_df=over_balanced_df.append(NEW_DF[NEW_DF.dialect==lebel].sample(25000, replace=True)).reset_index(drop=True)\nover_balanced_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:34.496216Z","iopub.execute_input":"2022-03-14T16:00:34.496541Z","iopub.status.idle":"2022-03-14T16:00:35.190424Z","shell.execute_reply.started":"2022-03-14T16:00:34.496504Z","shell.execute_reply":"2022-03-14T16:00:35.189565Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"balanced_df = pd.concat([over_balanced_df, under_balanced_df], axis=0).reset_index(drop=True)\nbalanced_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:35.191906Z","iopub.execute_input":"2022-03-14T16:00:35.192237Z","iopub.status.idle":"2022-03-14T16:00:35.236684Z","shell.execute_reply.started":"2022-03-14T16:00:35.192200Z","shell.execute_reply":"2022-03-14T16:00:35.235775Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\ndialect_count_balanced = balanced_df.groupby('dialect', as_index=False).count().sort_values(['text'],ascending=False,)\nFig1 =px.bar(x=dialect_count_balanced.dialect ,y=dialect_count_balanced.text, template='plotly_dark')\n\nFig1.update_traces(  texttemplate=\"%{y}\",textposition='outside')\nFig1.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:35.238015Z","iopub.execute_input":"2022-03-14T16:00:35.238369Z","iopub.status.idle":"2022-03-14T16:00:35.387427Z","shell.execute_reply.started":"2022-03-14T16:00:35.238322Z","shell.execute_reply":"2022-03-14T16:00:35.386604Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Text preprocessing","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\ndef text_preprocessing(text):\n\n        \n    \"\"\"\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n    \n    # Remove'@name'\n    text = re.sub(\"@\\S+\", '', text)\n    #Remove URLs\n    text = re.sub('^https?:\\/\\/.*[\\r\\n]*', '', text)\n    # Remove punctuation\n    text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?[\\]^_`{|}~]','',text)\n    #Remove newline character\n    text = re.sub('\\n', '', text)\n    #Remove emoji \n    text = re.sub(\"[\" u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                                u\"\\U00002500-\\U00002BEF\"  # chinese char\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   u\"\\U0001f926-\\U0001f937\"\n                                   u\"\\U00010000-\\U0010ffff\"\n                                   u\"\\u2640-\\u2642\"\n                                   u\"\\u2600-\\u2B55\"\n                                   u\"\\u200d\"\n                                   u\"\\u23cf\"\n                                   u\"\\u23e9\"\n                                   u\"\\u231a\"\n                                   u\"\\ufe0f\"  # dingbats\n                                   u\"\\u3030\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", '', text)\n    # replcae (أ,آ,إ) by (ا)\n    text = re.sub('[أإآ]', 'ا', text)\n    # replace (ة) by (ه)\n    text = re.sub('[ة]', 'ه', text)\n    text = text.replace('هه', 'ه')\n    text = text.replace('وو', 'و')\n    text = text.replace('يي', 'ي')\n    text = text.replace('اا', 'ا')\n    # remove multi spaces\n    text = re.sub(' +', ' ', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:35.388708Z","iopub.execute_input":"2022-03-14T16:00:35.389205Z","iopub.status.idle":"2022-03-14T16:00:35.398393Z","shell.execute_reply.started":"2022-03-14T16:00:35.389163Z","shell.execute_reply":"2022-03-14T16:00:35.397598Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"balanced_df['Clean_text']=balanced_df['text'].apply(text_preprocessing)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:36.448468Z","iopub.execute_input":"2022-03-14T16:00:36.448802Z","iopub.status.idle":"2022-03-14T16:00:48.723221Z","shell.execute_reply.started":"2022-03-14T16:00:36.448771Z","shell.execute_reply":"2022-03-14T16:00:48.722365Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"balanced_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:48.724704Z","iopub.execute_input":"2022-03-14T16:00:48.725034Z","iopub.status.idle":"2022-03-14T16:00:48.735318Z","shell.execute_reply.started":"2022-03-14T16:00:48.724998Z","shell.execute_reply":"2022-03-14T16:00:48.734191Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = balanced_df.text\ny = balanced_df.dialect\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42,stratify=y,test_size=0.3)\nX_train, X_val, y_train1, y_val = train_test_split(x_train, y_train, random_state=42,stratify=y_train,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:48.737704Z","iopub.execute_input":"2022-03-14T16:00:48.738065Z","iopub.status.idle":"2022-03-14T16:00:50.088797Z","shell.execute_reply.started":"2022-03-14T16:00:48.738031Z","shell.execute_reply":"2022-03-14T16:00:50.087761Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# RandomForest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nRegressor = RandomForestClassifier(n_estimators=400,max_depth=35,random_state=128,max_features='sqrt',bootstrap=False)\n\npipe_reg = Pipeline([\n    ('tfid', TfidfVectorizer(ngram_range=(1, 3),)),  \n    ('model', Regressor)])\n\npipe_reg.fit(X_train, y_train1)\n\n\ny_pred_pipe_reg_tr = pipe_reg.predict(X_train)\ny_pred_pipe_reg_val = pipe_reg.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:00:50.090192Z","iopub.execute_input":"2022-03-14T16:00:50.090519Z","iopub.status.idle":"2022-03-14T16:08:26.364619Z","shell.execute_reply.started":"2022-03-14T16:00:50.090478Z","shell.execute_reply":"2022-03-14T16:08:26.363759Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**For validation dataset**","metadata":{}},{"cell_type":"code","source":"target = NEW_DF['dialect'].astype('category')\nprint('categories: {}'.format(target.cat.categories))\nfrom sklearn import metrics\nprint(metrics.classification_report(y_val, y_pred_pipe_reg_val))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:08:26.365923Z","iopub.execute_input":"2022-03-14T16:08:26.366310Z","iopub.status.idle":"2022-03-14T16:08:30.261271Z","shell.execute_reply.started":"2022-03-14T16:08:26.366271Z","shell.execute_reply":"2022-03-14T16:08:30.259769Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**For test dataset**","metadata":{}},{"cell_type":"code","source":"y_pred_pipe_reg_test= pipe_reg.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:08:30.263004Z","iopub.execute_input":"2022-03-14T16:08:30.263349Z","iopub.status.idle":"2022-03-14T16:09:26.117587Z","shell.execute_reply.started":"2022-03-14T16:08:30.263302Z","shell.execute_reply":"2022-03-14T16:09:26.116730Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:15:32.118658Z","iopub.execute_input":"2022-03-14T16:15:32.119125Z","iopub.status.idle":"2022-03-14T16:15:32.128652Z","shell.execute_reply.started":"2022-03-14T16:15:32.119084Z","shell.execute_reply":"2022-03-14T16:15:32.127770Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"target = NEW_DF['dialect'].astype('category')\nprint('categories: {}'.format(target.cat.categories))\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred_pipe_reg_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:09:26.119052Z","iopub.execute_input":"2022-03-14T16:09:26.119436Z","iopub.status.idle":"2022-03-14T16:09:34.002419Z","shell.execute_reply.started":"2022-03-14T16:09:26.119384Z","shell.execute_reply":"2022-03-14T16:09:34.001533Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Approach\n- Given that the Random Forest Classifier model wasn't generalizing well for other datasets (possibly overfitting), I decided to try a DL approach using a pretrained model (i.e: increasing the dataset as a way of overcoming overfitting). For that I chose to use the [Multi-dialect-Arabic-BERT](https://github.com/mawdoo3/Multi-dialect-Arabic-BERT) \n> The models were pretrained  on 10M arabic tweets\n","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:23.859035Z","iopub.execute_input":"2022-03-14T11:15:23.859406Z","iopub.status.idle":"2022-03-14T11:15:24.835560Z","shell.execute_reply.started":"2022-03-14T11:15:23.859370Z","shell.execute_reply":"2022-03-14T11:15:24.834764Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:24.837067Z","iopub.execute_input":"2022-03-14T11:15:24.837411Z","iopub.status.idle":"2022-03-14T11:15:25.443554Z","shell.execute_reply.started":"2022-03-14T11:15:24.837376Z","shell.execute_reply":"2022-03-14T11:15:25.442741Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1\"> </a>\n### Multi-dialect-Arabic-BERT\nCode adapted from https://skimai.com/fine-tuning-bert-for-sentiment-analysis/","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:25.445223Z","iopub.execute_input":"2022-03-14T11:15:25.445481Z","iopub.status.idle":"2022-03-14T11:15:29.542279Z","shell.execute_reply.started":"2022-03-14T11:15:25.445456Z","shell.execute_reply":"2022-03-14T11:15:29.541371Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.1\"> </a>\n##### Preprocessing","metadata":{}},{"cell_type":"code","source":"# Define preprocessing util function\ndef text_preprocessing(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n  \n\n    # Normalize unicode encoding\n    text = unicodedata.normalize('NFC', text)\n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    #Remove URLs\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '<URL>', text)\n\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:29.543784Z","iopub.execute_input":"2022-03-14T11:15:29.544129Z","iopub.status.idle":"2022-03-14T11:15:29.549940Z","shell.execute_reply.started":"2022-03-14T11:15:29.544094Z","shell.execute_reply":"2022-03-14T11:15:29.549133Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Create a function to tokenize a set of texts\nimport emoji\nimport unicodedata\ndef preprocessing_for_bert(data, text_preprocessing_fn = text_preprocessing ):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    # Create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n    tokenizer = AutoTokenizer.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\n\n    # For every sentence...\n    for i,sent in enumerate(data):\n        # `encode_plus` will:\n        #    (1) Tokenize the sentence\n        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        #    (3) Truncate/Pad sentence to max length\n        #    (4) Map tokens to their IDs\n        #    (5) Create attention mask\n        #    (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing_fn(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate/pad\n            padding='max_length',        # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            return_attention_mask=True,     # Return attention mask\n            truncation = True \n            )\n        \n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:29.551330Z","iopub.execute_input":"2022-03-14T11:15:29.551894Z","iopub.status.idle":"2022-03-14T11:15:29.578153Z","shell.execute_reply.started":"2022-03-14T11:15:29.551857Z","shell.execute_reply":"2022-03-14T11:15:29.577402Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(balanced_df.dialect)\n\ny_train_labeled = le.transform(balanced_df.dialect)\nbalanced_df['labeled']=y_train_labeled","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:29.579333Z","iopub.execute_input":"2022-03-14T11:15:29.579896Z","iopub.status.idle":"2022-03-14T11:15:29.717031Z","shell.execute_reply.started":"2022-03-14T11:15:29.579858Z","shell.execute_reply":"2022-03-14T11:15:29.716282Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = balanced_df.text\ny = balanced_df.labeled\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42,stratify=y,test_size=0.3)\nX_train, X_val, y_train1, y_val = train_test_split(x_train, y_train, random_state=42,stratify=y_train,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:29.718505Z","iopub.execute_input":"2022-03-14T11:15:29.719079Z","iopub.status.idle":"2022-03-14T11:15:30.305472Z","shell.execute_reply.started":"2022-03-14T11:15:29.719043Z","shell.execute_reply":"2022-03-14T11:15:30.304585Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import re\n# Specify `MAX_LEN`\nMAX_LEN =  280\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:15:30.309918Z","iopub.execute_input":"2022-03-14T11:15:30.310176Z","iopub.status.idle":"2022-03-14T11:17:14.893379Z","shell.execute_reply.started":"2022-03-14T11:15:30.310150Z","shell.execute_reply":"2022-03-14T11:17:14.892375Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.2\"> </a>\n##### Create data loaders for test and validation sets","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train1.tolist())\nval_labels = torch.tensor(y_val.tolist())\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:17:14.894965Z","iopub.execute_input":"2022-03-14T11:17:14.895293Z","iopub.status.idle":"2022-03-14T11:17:14.943132Z","shell.execute_reply.started":"2022-03-14T11:17:14.895258Z","shell.execute_reply":"2022-03-14T11:17:14.942348Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.3\"> </a>\n##### Define model initialization class and functions","metadata":{}},{"cell_type":"code","source":"%%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in =768\n        H, D_out = 50, 18\n\n        # Instantiate BERT model\n        self.bert = AutoModel.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:17:14.944377Z","iopub.execute_input":"2022-03-14T11:17:14.944710Z","iopub.status.idle":"2022-03-14T11:17:14.953360Z","shell.execute_reply.started":"2022-03-14T11:17:14.944681Z","shell.execute_reply":"2022-03-14T11:17:14.952366Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom torch.optim import SparseAdam, Adam\ndef initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(params=list(bert_classifier.parameters()),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:17:14.954780Z","iopub.execute_input":"2022-03-14T11:17:14.955298Z","iopub.status.idle":"2022-03-14T11:17:19.347388Z","shell.execute_reply.started":"2022-03-14T11:17:14.955259Z","shell.execute_reply":"2022-03-14T11:17:19.346542Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.4\"> </a>\n##### Define model train and evaluate functions","metadata":{}},{"cell_type":"code","source":"import random\nimport time\nimport torch\nimport torch.nn as nn\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:17:19.348815Z","iopub.execute_input":"2022-03-14T11:17:19.349163Z","iopub.status.idle":"2022-03-14T11:17:19.368697Z","shell.execute_reply.started":"2022-03-14T11:17:19.349127Z","shell.execute_reply":"2022-03-14T11:17:19.367546Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**Define prediction and test set evaluation functions**","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:17:19.370133Z","iopub.execute_input":"2022-03-14T11:17:19.370614Z","iopub.status.idle":"2022-03-14T11:17:19.379896Z","shell.execute_reply.started":"2022-03-14T11:17:19.370573Z","shell.execute_reply":"2022-03-14T11:17:19.379065Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\n\n\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 18)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n\n    # Get accuracy over the test set\n    y_pre=[]\n    for y in probs:\n        y_pre.append(np.argmax(y))\n\n    y_true_inverse=list(le.inverse_transform(y_true))\n    y_pre_inverse=list(le.inverse_transform(y_pre))\n    print(metrics.classification_report(y_true_inverse, y_pre_inverse))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:17:19.381640Z","iopub.execute_input":"2022-03-14T11:17:19.383004Z","iopub.status.idle":"2022-03-14T11:17:19.390244Z","shell.execute_reply.started":"2022-03-14T11:17:19.382962Z","shell.execute_reply":"2022-03-14T11:17:19.389511Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.5\"> </a>\n##### Initialize and train model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nset_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T11:17:19.391360Z","iopub.execute_input":"2022-03-14T11:17:19.391612Z","iopub.status.idle":"2022-03-14T15:23:00.499915Z","shell.execute_reply.started":"2022-03-14T11:17:19.391577Z","shell.execute_reply":"2022-03-14T15:23:00.492571Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**Saving & loading the model**","metadata":{}},{"cell_type":"code","source":"torch.save(bert_classifier, 'model_DL.pt')\nmodel_DL= torch.load('model_DL.pt')","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:22:19.785777Z","iopub.execute_input":"2022-03-14T16:22:19.786091Z","iopub.status.idle":"2022-03-14T16:22:19.789850Z","shell.execute_reply.started":"2022-03-14T16:22:19.786063Z","shell.execute_reply":"2022-03-14T16:22:19.788777Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"**Predict and evaluate validation subset**","metadata":{}},{"cell_type":"code","source":"probs_bert_classifier = bert_predict(bert_classifier, val_dataloader)\nevaluate_roc(probs_bert_classifier, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:20:58.335380Z","iopub.execute_input":"2022-03-14T16:20:58.335718Z","iopub.status.idle":"2022-03-14T16:21:06.138351Z","shell.execute_reply.started":"2022-03-14T16:20:58.335683Z","shell.execute_reply":"2022-03-14T16:21:06.137404Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Predict and evaluate Test subset**","metadata":{}},{"cell_type":"code","source":"# Run `preprocessing_for_bert` on the test set\ntest_inputs, test_masks = preprocessing_for_bert(x_test)\n# x_test,  y_test\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:22:29.377465Z","iopub.execute_input":"2022-03-14T16:22:29.377851Z","iopub.status.idle":"2022-03-14T16:22:29.382357Z","shell.execute_reply.started":"2022-03-14T16:22:29.377794Z","shell.execute_reply":"2022-03-14T16:22:29.381390Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, test_dataloader)\nevaluate_roc(probs, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T16:20:58.335380Z","iopub.execute_input":"2022-03-14T16:20:58.335718Z","iopub.status.idle":"2022-03-14T16:21:06.138351Z","shell.execute_reply.started":"2022-03-14T16:20:58.335683Z","shell.execute_reply":"2022-03-14T16:21:06.137404Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.11\"> </a>\n##### Summary of performance on test datasets\n\n| Model | Accuracy (%)\n| :---: | :---: |\n| RandomForestClassifier | 51 \n| Multi-dialect-Arabic-BERT | 64\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}